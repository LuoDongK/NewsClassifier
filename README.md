# 数据预处理

### 由于数据全部被数字化，因此数据预处理部分能做的不多。统计各个单词在所有新闻中的出现频率，3750 648 900加入停用词。

### 将原始数据集进行划分，考虑到样本的分布，按照样本标签的比例划分训练集与验证集。

# LSTM + CrossEntropyLoss

### 模型本身其实就是一个LSTM的多分类问题，并不难。但由于内存的限制，很多地方做了一些处理以优化内存。

# 结果

### embedding_dim = 50, hidden_layer = 80, max_epoch = 20, accuracy = 95.45%, F1_score = 0.9457